# Репликация в ClickHouse: теория и настройка

## Теория
**Репликация** в ClickHouse позволяет поддерживать отказоустойчивость и горизонтальное масштабирование за счёт хранения копий данных на нескольких узлах.
Основой репликации в ClickHouse являются механизмы распределённых таблиц и таблиц с движком `ReplicatedMergeTree`.
Репликация работает на уровне отдельных таблиц, а не всего сервера. То есть, на сервере могут быть расположены одновременно реплицируемые и не
реплицируемые таблицы.
- Репликация не зависит от шардирования. На каждом шарде репликация работает независимо.
- Реплицируются сжатые данные запросов `INSERT`, `ALTER`.
- Запросы `CREATE`, `DROP`, `ATTACH`, `DETACH` и `RENAME` выполняются на одном сервере и не реплицируются:
    - Запрос `CREATE TABLE` создаёт новую реплицируемую таблицу на том сервере, где его выполнили. Если таблица уже существует на других серверах, запрос добавляет новую реплику.
    - `DROP TABLE` удаляет реплику, расположенную на том сервере, где выполняется запрос.
    - Запрос `RENAME` переименовывает таблицу на одной реплик. Другими словами, реплицируемые таблицы на разных репликах могут называться по-разному.
- ClickHouse хранит метаинформацию о репликах в Apache ZooKeeper.

Репликация `асинхронная`, `мультимастер`. Запросы `INSERT` и `ALTER` можно направлять на любой доступный сервер. Данные вставятся на сервер,
где выполнен запрос, а затем скопируются на остальные серверы. В связи с асинхронностью, только что вставленные данные появляются на остальных репликах с 
небольшой задержкой. Если часть реплик недоступна, данные на них запишутся тогда, когда они станут доступны. Если реплика доступна, то задержка составляет 
столько времени, сколько требуется для передачи блока сжатых данных по сети. Количество потоков для выполнения фоновых задач можно задать с помощью настройки
 background_schedule_pool_size.

### Основные компоненты репликации
1. **ZooKeeper**

    Репликация в ClickHouse требует внешнего сервиса координации — ZooKeeper, который используется для управления:
    - Метаданными о репликах.
    - Очередью заданий для синхронизации данных.
    - Лидером для обработки слияний (merge) и других операций.

2. **ReplicatedMergeTree**

    Таблицы с движком ReplicatedMergeTree (или его производными, например, ReplicatedReplacingMergeTree, ReplicatedSummingMergeTree) автоматически управляют репликами:
    - Каждая реплика подключается к ZooKeeper, чтобы координировать свою рабо-ту с другими репликами.
    - Данные автоматически синхронизируются между репликами.

### Как работает репликация?
1. **Создание таблиц с репликацией**

    Для создания реплицируемой таблицы используется движок ReplicatedMergeTree. Основные параметры:
    - `zookeeper_path` — путь к метаданным таблицы в ZooKeeper.
    - `replica_name` — уникальное имя реплики.

    **zookeeper_path** (путь к таблице в ZooKeeper): этот параметр указыва-ет путь к таблице в ZooKeeper, где хранятся метаданные и информация о репликах. Например, если у вас есть таблица my_table, то путь к ней в ZooKeeper может быть /clickhouse/tables/my_table.
    - Пример: `ENGINE = ReplicatedMergeTree('/clickhouse/tables/my_table', 'replica_name')`

    **replica_name** (имя реплики в ZooKeeper): этот параметр определяет имя конкретной реплики в ZooKeeper. Каждая реплика должна иметь уни-кальное имя. Например, replica_name может быть replica1, replica2 и т.д.
    - Пример: `ENGINE = ReplicatedMergeTree('/clickhouse/tables/my_table', 'replica1')`

2. **Механизм записи**

    - Данные пишутся в одну из реплик.
    - Реплика создаёт файл данных (part) и запись об этом файле в ZooKeeper.
    - Другие реплики считывают эту информацию из ZooKeeper и скачивают недостающие части данных.

3. **Синхронизация реплик**

    Реплики синхронизируются через механизмы:
    - Пуллинга данных: реплики проверяют ZooKeeper на наличие новых данных (частей) и скачивают их.
    - Очереди операций: ZooKeeper управляет задачами для каждой реплики, например, заданиями на скачивание, удаление или слияние данных.

4. **Слияние данных**

    Слияние (merge) происходит на каждой реплике независимо. Лиде-ром процесса выступает одна из реплик, которая сообщает остальным о завершении слияния.

5. **Чтение данных**

    Репликация не влияет на чтение данных. Все реплики могут обслуживать запросы на чтение, что увеличивает производительность в распределённых системах.

6. **Автоматическое восстановление**

    Если одна из реплик выходит из строя, новая реплика может быть создана из оставшихся данных:
    - Подключается к ZooKeeper.
    - Скачивает все недостающие данные с других реплик.

## Настройка

### 1. Установка кластера ZooKeeper

Используйте скрипт для установка и настройки кластера ZooKeeper в репозитории [PetrovEvgenyS/zookeeper](https://github.com/PetrovEvgenyS/zookeeper)

### 2. Создадим кластер из 2-х нод ClickHouse.

#### 2.1. Устанавливаем ClickHouse
- Выполняем установку ClickHouse на все машины кластера используя скрипт для установки: [setup_clickhouse.sh](/setup_clickhouse.sh)

#### 2.2. Конфигурация ClickHouse сервера и репликации

Для репликации данных в кластере нас интересуют следующие параметры в конфиг-файле `/etc/clickhouse-server/config.xml`:
- `zookeeper` — конфигурация Zookeeper для ClickHouse кластера.
- `macros` — макросы для подстановки параметров реплицируемых таблиц.

Нужно, чтобы данные параметры были пустыми, т.к. мы используем файл с подстановками.

Настроим конфигурацию сервера [my-config-replica.xml](/replication/my-config-replica.xml):

- Сохраняем конфигурацию сервера в - `/etc/clickhouse-server/config.d/my-config-replica.xml`

Данный кластер будет состоять из 1-го шарда и 2-х реплик. То есть данные между шардами делиться не будут, поскольку у нас всего лишь один шард.
Но они будут реплицироваться в рамках одного шарда, по-скольку у нас две реплики в одном шарде.
То есть каждая нода ClickHouse в нашем случае является репликой.
Полная конфигурация, выглядит следующим образом: [cluster-replica.xml](/replication/cluster-replica.xml)

- Сохраняем конфигурацию репликации в - `/etc/clickhouse-server/cluster-replica.xml`

#### 2.3. Меняем владельца

```bash
chown clickhouse:clickhouse /etc/clickhouse-server/config.d/my-config-replica.xml /etc/clickhouse-server/cluster-replica.xml
```

#### 2.4. Разбор cluster-replica.xml

Структуру нашего кластера мы отобразили в параметре `clickhouse_cluster_replica`.

Параметр `internal_replication` отвечает за то, будет ли дублировать ClickHouse запросы на запись или редактирование данных автоматически на все реплики 
или нет при вставке в `distributed` таблицу. То есть, когда вы вставляете данные в `distributed` таблицу, она сама может увидеть все реплики в шарде 
и продублировать запрос на вставку на все реплики этого шарда.

Второй же вариант — это настройка репликации для таблиц через `ZooKeeper` своими руками. То есть мы создадим две таблицы и скажем, что они реплицируемые. 
В этом случае нам не потребуется репликация через `distributed`, поэтому cтавим **true** в данном параметре.


В конфигурации параметра `ZooKeeper` мы указываем кол-во нод `ZooKeeper`. В данном случае, имеем 3 ноды, указываем адрес и порт соответственно.
Атрибут `index` отвечает за `ZooKeeper ID`.

Последний блок — это `макросы`. Они нужны для автоматической подстановки значений при создании реплицированных таблиц (через `sql`) в ClickHouse.
Это позволяет избегать опечаток и других ошибок. Макросы привязаны к хосту, где запущен ClickHouse, а поэтому будут иметь разные значения на 
разных нодах кластера.

### 3. Тестирование кластера ClickHouse
- Теперь, когда мы закончили с конфигурацией, нужно проверить, все ли корректно работает. Для начала убедимся, что ClickHouse обнаружил и успешно прочитал наш `cluster-replica.xml` конфиг-файл.
- Перезапустим сервис для применения конфига и посмотрим логи, нас интересует строка `Including configuration file '/etc/clickhouse-server/cluster-replica.xml'.`. Если в логах есть это сообщение, а в логах ошибок ClickHouse нет замечаний — идем дальше.
    ```bash
    systemctl restart clickhouse-server
    systemctl status clickhouse-server
    ```

### 4. Создание реплицированных таблиц

Для тестирования кластера, создадим реплицируемые таблицы. Напомню, что таблицы должны находиться на всех нодах и называться одинаково.

```sql
CREATE TABLE example_table ON CLUSTER my_cluster
(
    id UInt64,
    name String,
    date Date
)
ENGINE = ReplicatedMergeTree('/clickhouse/tables/{cluster}/{shard}/{database}/example_table', '{replica}')
PARTITION BY date
ORDER BY (id);
```

`ON CLUSTER my_cluster` указывает, что таблица будет создана на всех узлах кластера с именем `my_cluster`.
В этом запросе есть секция `ON CLUSTER`, которая позволяет выполнить запрос на всех узлах кластера. Для подстановки значений для запроса будут использоваться 
макросы, которые мы описали ранее. Также необходимо подключение к `ZooKeeper` серверу.

Путь к таблице в ZooKeeper должен быть разным для каждой реплицируемой таблицы. В том числе, для таблиц на разных шардах, должны быть разные пути.
В данном случае, путь состоит из следующих частей:

- `/clickhouse/tables/` — общий префикс. Рекомендуется использовать именно его.

- `{shard}` — идентификатор шарда (значение берётся из макроса) и т.д..

- `example_table` - имя узла для таблицы в ZooKeeper. Разумно делать его таким же, как имя таблицы. Оно указывается явно, так как, в отличие от имени таблицы,
 оно не меняется после запроса `RENAME`.

- `replica` (имя реплики) — то, что идентифицирует разные реплики одной и той же таблицы. Можно использовать для него имя сервера. Впрочем, достаточно,
чтобы имя было уникально лишь в пределах каждого шарда.

**Важно: Каждая реплика должна использовать одинаковый путь в ZooKeeper (/clickhouse/tables/example_table), но уникальное имя реплики (replica1, replica2 и т. д.).**

Здесь мы используем движок `ReplicatedMergeTree`, который ничем не отличается от движка `MergeTree`, но позволяет дополнительно реплицировать данные.
В параметрах мы указываем путь к таблице в `Zookeeper`, а также название реплики. Реплицируемые таблицы должны иметь одинаковый путь в `Zookeeper`.
То есть если вы хотите, чтобы 5 нод реплицировали одну и ту же таблицу - то путь в зукипере у них будет один и тот же - 
`/clickhouse/{cluster}/tables/posts_replicated`, а вот индекс реплики будет отличаться.

Значения переменных `{cluster}` и `{replica}` автоматически подставятся для каждой ноды на кластере из макросов.

Собственно, на этом можно заканчивать рассказ про репликацию, поскольку она уже настроена для данной таблицы.

### 5. Проверка реплицикации таблиц

Вставляем данные в таблицу на любой ноде:

```sql
INSERT INTO example_table (id, name, date) VALUES 
(1, '2022 Toyota Camry', '2024-11-14'),
(2, '2019 BMW X5', '2024-11-15'),
(3, '2023 Tesla Model S', '2024-11-16'),
(4, '2021 Ford F-150', '2024-11-17'),
(5, '2020 Mercedes-Benz E-Class', '2024-11-18');
```

Проверяем на каждой ноде, убеждаем, что данные, везде одинаковы.
```sql
SELECT * FROM example_table;
```
