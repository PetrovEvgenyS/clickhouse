# Шардирование в ClickHouse: теория и настройка

## Хранение файлов:
- `cluster.xml` - /etc/clickhouse-server/cluster.xml
- `my-config.xml` - /etc/clickhouse-server/config.d/my-config.xml

## Что такое шардирование в ClickHouse?
**Шардирование** (sharding) - это метод горизонтального разделения данных между несколькими серверами (шардами), где каждый шард содержит часть данных. В ClickHouse шардирование используется для:
- Масштабирования обработки запросов
- Увеличения общей пропускной способности системы
- Распределения нагрузки между серверами

### Как работает шардирование в ClickHouse
- **Распределение данных**: пи вставке данных они распределяются между шардами согласно выбранному алгоритму.
- **Распределённые запросы**: при выполнении запроса к распределённой таблице, ClickHouse отправляет запрос на все шарды и объединяет результаты.
- **Локальная обработка**: каждый шард обрабатывает свою часть данных независимо.

### Основные компоненты шардирования
- **Distributed таблица** - виртуальная таблица, которая не хранит данные, а только координирует запросы к шардам.
- **Локальные таблицы** - реальные таблицы на каждом шарде, где хранятся данные.
- **Кластер** - группа серверов, участвующих в шардировании.

## Настройка шардирования

### 1. Создадим кластер из 2-х нод ClickHouse.

#### 1.1. Устанавливаем ClickHouse
- Выполняем установку ClickHouse на все машины кластера используя скрипт для установки: [setup_clickhouse.sh](/setup_clickhouse.sh)

#### 1.2. Конфигурация кластера
- В конфигурационном файле `/etc/clickhouse-server/config.xml`, убедитесь, что параметр `remote_servers` пустой. Если нет, то удалите содержимое, по-скольку мы вынесем конфигурацию кластера в отдельный файл с подстановками.

- Теперь перейдем к конфигурации самого кластера. Наш кластер состоит из 2-х шардов по 1-й реплике в каждом. Каждая нода ClickHouse является репликой. Для начала мы обойдемся без репликации данных. Полная конфигурация [cluster.xml](/sharding/cluster.xml) выглядит следующим образом.

    - Сохраняем конфигурацию кластера в - `/etc/clickhouse-server/cluster.xml`

#### 1.3. Конфигурация сервера
- Настроим конфигурацию сервера [my-config.xml](/sharding/my-config.xml):

    - Сохраняем конфигурацию сервера в - `/etc/clickhouse-server/config.d/my-config.xml`

#### 1.4. Меняем владельца:

```bash
chown clickhouse:clickhouse /etc/clickhouse-server/cluster.xml /etc/clickhouse-server/config.d/my-config.xml
```

- Настраиваем вторую ноду, точно также.

### 2. Тестирование кластера ClickHouse
- Теперь, когда мы закончили с конфигурацией, нужно проверить, все ли корректно работает. Для начала убедимся, что ClickHouse обнаружил и успешно прочитал наш `cluster.xml` конфиг-файл.
- Перезапустим сервис для применения конфига и посмотрим логи, нас интересует строка `Including configuration file '/etc/clickhouse-server/cluster.xml'.`. Если в логах есть это сообщение, а в логах ошибок ClickHouse нет замечаний — идем дальше.
    ```bash
    systemctl restart clickhouse-server
    systemctl status clickhouse-server
    ```

### 3. Создание таблиц в кластере
- Чтобы протестировать работоспособность кластера, создадим на каждой ноде таблицу.
    ```sql
    CREATE TABLE posts
    (
    id Int64,
    title String,
    description String,
    content String,
    date Date
    )
    ENGINE = MergeTree()
    PARTITION BY date
    ORDER BY id;
    ```

### 4. Создание Distributed таблиц
- **Distributed** (распределенная) таблица не хранит никаких данных и по сути является виртуальной. **Виртуальные** — это те, которое фактически не хранят никаких данных, а забирают их из других источников. Её основная задача — распределение запросов на все локальные таблицы в узлах кластера. К примеру, при запросе `SELECT Distributed` таблица переписывает запрос, выбирает нужные удаленные узлы и отправляет им запрос. При этом стоить заметить, что сам запрос максимально обрабатывается на стороне узлов. Как только `Distributed` таблица получает данные она их объединяет.

- Теперь к созданию Distributed таблиц. Они должны иметь такую же структуру, как и таблицы, которые мы создали ранее, но другой движок для запросов:
    ```sql
    CREATE TABLE posts_distributed
    (
    id Int64,
    title String,
    description String,
    content String,
    date Date
    )
    ENGINE = Distributed('my_cluster', 'default', 'posts', rand());
    ```

- В параметрах движка таблицы мы указываем:
    - имя кластера `my_cluster`,
    - база данных с таблицей `default`,
    - имя таблицы `posts`,
    - ключ шардирования `rand()`.

### 5. Вставка данных
Теперь пришло время вставки данных в ClickHouse. Мы можем вставлять данные либо в конкретный шард, либо используя distributed таблицу. Для начала вставим данные только в один шард, в первую ноду.

```sql
INSERT INTO posts (id, title, description, date) VALUES 
(1, '2022 Toyota Camry', 'Седан среднего класса', '2024-11-14'),
(2, '2019 BMW X5', 'Люксовый кроссовер', '2024-11-15'),
(3, '2023 Tesla Model S', 'Электрический седан', '2024-11-16'),
(4, '2021 Ford F-150', 'Пикап полноразмерный', '2024-11-17'),
(5, '2020 Mercedes-Benz E-Class', 'Бизнес-класс седан', '2024-11-18');
```

```sql
SELECT count() FROM posts;
```
Результат: `5`

Как мы видим, данные вставились. Теперь сделаем выборку со второй ноды:
```sql
SELECT count() FROM posts;
```
Результат: `0`

```sql
SELECT count() FROM posts_distributed
```
Результат: `5`

Видно, что на второй ноде данных в табличке `posts` нету, поскольку мы вставили их прямо в шард. Но они есть в `posts_distributed`, поскольку эта таблица смотрит на оба шарда.

- Отлично, мы успешно реализовали ClickHouse кластер!
